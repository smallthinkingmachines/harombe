\documentclass[11pt,a4paper]{article}

% ============================================================
% Packages
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{natbib}
\usepackage{fancyvrb}
\usepackage{orcidlink}

% ============================================================
% Styling
% ============================================================
\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  citecolor=green!50!black,
  urlcolor=blue!70!black,
}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!5},
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red!60!black},
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=5pt,
  xleftmargin=2em,
}

% ============================================================
% Title
% ============================================================
\title{%
  \textbf{The Capability-Container Pattern:\\
  Infrastructure-Level Security for Autonomous AI Agents}%
}
\author{
  Ricardo Ledan\,\orcidlink{0009-0004-3041-7656}
}
\date{February 2026}

% ============================================================
\begin{document}
\maketitle

% ============================================================
% Abstract
% ============================================================
\begin{abstract}
Autonomous AI agents that invoke external tools via protocols like MCP (Model Context Protocol) present a novel attack surface: the agent-tool boundary. Existing frameworks rely on prompt-level safeguards or protocol-level trust, both of which are insufficient against adversarial inputs, tool poisoning, and credential leakage. We present the \emph{Capability-Container Pattern}, an infrastructure-level security architecture where agents never directly access tools. All tool invocations flow through a mediation gateway into isolated containers, each provisioned with only the capabilities it requires. We describe the threat model, detail six defense-in-depth layers (container isolation, network egress filtering, credential vaults, audit logging, secret scanning, and human-in-the-loop gates), and provide benchmark results from a reference implementation (self-evaluated with synthetic test data; see \S\ref{sec:eval-limitations}). Our evaluation uses a 1{,}534-sample corpus---900 synthetic secrets (100 per type, 9 types), 514 benign samples across 7 categories, and 120 adversarial evasion samples---with Clopper--Pearson confidence intervals and comparative analysis against \texttt{detect-secrets}. Gateway logic overhead is ${\sim}0.025$\,ms (0.005\% of LLM inference latency, cross-run 95\% CI: [0.022, 0.028]), with 100\% secret detection rate (CI: [0.996, 1.000]), $F_1 = 0.991$ vs.\ 0.779 for \texttt{detect-secrets}, and 0.19\% false positive rate (CI: [0.000, 0.011]). The pattern provides defense coverage for 6/7 reconstructed 2025 MCP breach scenarios---reducing the attack surface from unbounded host access to a narrow, auditable, policy-enforced channel.
\end{abstract}

% ============================================================
\section{Introduction \& Motivation}
\label{sec:introduction}
% ============================================================

The year 2025 saw the emergence of a new class of software vulnerability: the agent-tool boundary exploit. From WhatsApp chat history exfiltration in April (disclosed by Invariant Labs) to the Smithery registry vulnerability responsibly disclosed in October \citep{gitguardian2025smithery} that could have exposed 3{,}000+ MCP servers (no evidence of exploitation), a clear pattern emerged---the protocols designed for agent-tool communication provide no security enforcement at all.

\subsection{Problem Statement}

The Model Context Protocol (MCP) standardizes how AI agents invoke external tools via JSON-RPC 2.0. It defines message formats, tool discovery, and communication patterns. What it does \emph{not} define is:

\begin{itemize}[nosep]
  \item How tools are isolated from each other and the host
  \item How credentials are managed and injected
  \item How network access is constrained per-tool
  \item How tool invocations are audited
  \item How dangerous operations are gated on human approval
\end{itemize}

This gap between protocol specification and security enforcement is not merely theoretical. The 2025 MCP vulnerability timeline (\Cref{sec:threat-model}) demonstrates that every major disclosed vulnerability exploited this exact gap.

\subsection{Key Argument}

\textbf{Protocol-level security is fundamentally insufficient for autonomous AI agents.} An agent that can send arbitrary JSON-RPC messages can bypass any protocol-level restriction. Just as web security requires infrastructure enforcement (TLS, firewalls, WAFs) beyond HTTP, agent security requires infrastructure enforcement beyond MCP.

\subsection{Our Contribution}

We present the \textbf{Capability-Container Pattern}, an architectural pattern where:
\begin{enumerate}[nosep]
  \item Each tool runs in an isolated container with only the capabilities it needs
  \item All communication flows through a mediation gateway that enforces security policies
  \item Credentials are injected at runtime and never visible to the agent or tools
  \item Every operation is logged to an immutable audit trail
  \item High-risk operations require human approval before execution
\end{enumerate}

We provide: (a)~a formal threat model with attack surface analysis, (b)~a reference implementation (Harombe) with six defense-in-depth layers, (c)~benchmark results showing ${<}1$\,ms audit overhead, and (d)~a mapping of 2025 MCP breaches to specific defense layers that would have prevented them.

\subsection{Scope}

This paper focuses on the security architecture for \emph{tool execution} in autonomous AI agents. We do not address LLM alignment or output safety, training data poisoning, model extraction attacks, or multi-agent coordination security (future work).

% ============================================================
\section{Background \& Related Work}
\label{sec:background}
% ============================================================

\subsection{The Model Context Protocol (MCP)}

MCP is an open standard \citep{anthropic2024mcp} that defines how AI agents discover and invoke external tools via JSON-RPC 2.0. It specifies tool discovery, tool invocation with typed parameters, transport (stdio or streamable-HTTP), and content types (text, images, resource references). MCP has become the de facto standard for agent-tool communication, with thousands of community-built servers.

\textbf{What MCP does NOT specify:} isolation between tools, credential lifecycle management, network access control per tool, audit logging, or human approval workflows. This omission is by design---MCP is a wire protocol, not a security framework. The specification's own security best practices acknowledge that ``MCP servers should be launched with restricted access\ldots using platform-appropriate sandboxing technologies''---but provides no mechanism to enforce this.

\subsection{Autonomous AI Agent Frameworks}

Current agent frameworks focus on orchestration: task decomposition, multi-agent coordination, memory, and planning. Security is treated as an afterthought (\Cref{tab:frameworks}).

\begin{table}[htbp]
\centering
\caption{Security feature comparison of agent frameworks.}
\label{tab:frameworks}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Framework} & \textbf{Isolation} & \textbf{Network} & \textbf{Credentials} & \textbf{Audit} & \textbf{HITL} \\
\midrule
CrewAI       & None            & None           & None        & None              & None       \\
LangGraph    & None            & None           & None        & Cloud (LangSmith) & Node-level \\
AutoGen      & Optional Docker & None           & None        & None              & None       \\
OpenClaw     & Optional sandbox& None           & None        & None              & None       \\
\textbf{Ours}& \textbf{Per-tool}& \textbf{Per-container}& \textbf{Vault}& \textbf{Local+redact}& \textbf{Risk-based}\\
\bottomrule
\end{tabular}
\end{table}

AutoGen offers Docker-based code execution but does not extend isolation to all tools, does not filter network egress, and does not manage credentials. OpenClaw \citep{steinberger2025openclaw} is an open-source personal AI assistant that offers optional sandboxed operation and integrates with 50+ services, but provides no structured security enforcement---users choose between full system access or a basic sandbox, with no per-tool isolation, credential vaulting, network filtering, audit logging, or secret scanning.

\subsection{Container and Sandbox Security}

Container isolation (Docker, Podman) provides namespace separation for PIDs, network, filesystem, and IPC. However, containers share the host kernel---a known limitation extensively documented in the security literature.

\textbf{Empirical evidence of container weakness.}
Lin et al.\ \citep{lin2018container} found that 56.82\% of typical exploits succeed against default Docker configuration, and that namespace+cgroup isolation blocks only 21.62\% of privilege escalation attacks---while Seccomp+capabilities+MAC blocks 67.57\%. Jarkas et al.\ \citep{jarkas2025survey} cataloged 200+ container vulnerabilities across 47 exploit types and 11 attack vectors (2013--2024). Reeves et al.\ \citep{reeves2021analysis} analyzed 59 CVEs across 11 runtimes: all 9 documented escape exploits resulted from a host component leaked into the container; user namespace remapping stopped 7 of 9.

\textbf{Recent container escapes} include CVE-2024-21626 (runc ``Leaky Vessels'': file descriptor leak), CVE-2024-1086 (nf\_tables use-after-free; PoC reports 99.4\% success on KernelCTF images \citep{cve2024_1086poc}), CVE-2025-23266 (NVIDIA ``NVIDIAScape'': CVSS 9.0), CVE-2025-31133/52565/52881 (three runc mount-handling escapes), and DockerDash \citep{dockerdash2025}: an AI-specific attack where malicious Docker image labels exploit the Gordon AI $\rightarrow$ MCP Gateway pipeline.

\textbf{Namespace and eBPF limitations.} Sun et al.\ \citep{sun2018namespace} showed containers cannot independently define security policies due to shared kernel. He et al.\ \citep{he2023ebpf} demonstrated that eBPF tracing features break container isolation---found 5 vulnerable cloud services.

\textbf{gVisor} offers a middle ground: a user-space kernel reducing ${\sim}300$ syscalls to ${\sim}83$ (72\% reduction). Jang et al.\ \citep{jang2022gvisor} quantified gVisor as 4.2--7.5$\times$ more secure than standard runtimes. Young et al.\ \citep{young2019gvisor} measured 2.2--216$\times$ overhead depending on operation (simple syscalls to Gofer-mediated file I/O), but near-native CPU performance. Google's Kubernetes Agent Sandbox (2025) chose gVisor as its default runtime for AI agent workloads \citep{google2025agentsandbox}.

\textbf{Firecracker} microVMs \citep{agache2020firecracker} provide hardware-enforced isolation via KVM with 125\,ms boot time and ${\sim}3$\,MiB per-VM memory overhead. However, Weissman et al.\ \citep{weissman2023firecracker} showed limited Spectre/MDS protection, and Xiao et al.\ \citep{xiao2023microvm} demonstrated exploitable operation forwarding.

Our architecture uses Docker containers with aggressive hardening (custom Seccomp profiles, dropped capabilities, user namespace remapping, read-only filesystems) as the default, with gVisor as the recommended production layer. Defense-in-depth ensures that even if container isolation is breached, network egress filtering, credential vaults, and secret scanning provide additional barriers.

\subsection{Related Academic Work}
\label{sec:related-academic}

\textbf{AgentBound} \citep{buhler2025agentbound} introduces an access control framework inspired by Android's permission model, auto-generating security policies from MCP server source code with 80.9\% accuracy. Our work is complementary: AgentBound enforces permissions at the tool definition level (software enforcement), while we enforce at the infrastructure level (hardware-enforced container boundaries).

\textbf{Fault-Tolerant Sandboxing} \citep{yan2025sandbox} proposes transactional filesystem snapshots for AI coding agents, achieving 100\% interception of high-risk commands with 14.5\% overhead. This addresses filesystem safety but not network exfiltration, credential management, or multi-tool isolation.

\textbf{NVIDIA Safety Framework} \citep{ghosh2025nvidia} presents an operational risk taxonomy with auxiliary AI models for contextual risk management, validated against 10{,}000+ attack/defense traces. Their focus is risk classification and dynamic response; ours is infrastructure enforcement.

The Survey of Agentic AI and Cybersecurity \citep{survey2026agentic} provides critical empirical validation: ``over 75\% of malicious commands execute successfully without sandboxing, while container-based sandboxing blocks nearly all such commands.''

\Cref{tab:academic-comparison} summarizes the feature coverage of related academic systems.

\begin{table}[htbp]
\centering
\caption{Comparative feature coverage of academic agent security systems.}
\label{tab:academic-comparison}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{System} & \textbf{Container} & \textbf{Network} & \textbf{Cred.\ Vault} & \textbf{Audit} & \textbf{Secrets} & \textbf{HITL} \\
\midrule
IsolateGPT        & Per-app & ---     & ---          & ---      & ---      & ---      \\
AgentBound        & ---     & ---     & ---          & ---      & ---      & ---      \\
Fault-Tol.\ Sandbox& ---    & ---     & ---          & ---      & ---      & Partial  \\
NVIDIA Framework  & ---     & ---     & ---          & Partial  & ---      & Partial  \\
Fides             & Partial & ---     & ---          & ---      & ---      & ---      \\
ceLLMate          & ---     & ---     & ---          & ---      & ---      & ---      \\
MiniScope         & ---     & ---     & ---          & ---      & ---      & ---      \\
Progent           & ---     & ---     & ---          & Partial  & ---      & Partial  \\
\textbf{Ours}     & \textbf{Per-tool}& \textbf{Per-tool}& \textbf{Vault}& \textbf{Full}& \textbf{Pattern+entropy}& \textbf{Risk-based}\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Agent Isolation and Industry Frameworks}
\label{sec:related-isolation}

\textbf{IsolateGPT} \citep{wu2025isolategpt} is the closest prior work to ours. Wu et al.\ propose a hub-and-spoke architecture that transparently isolates LLM-integrated applications, preventing data leakage between apps sharing the same LLM backend. IsolateGPT focuses on \emph{data isolation}---ensuring one app's context cannot contaminate another's---via isolated LLM instances and API proxying. Our work addresses a complementary but distinct threat surface: we assume the LLM itself is trusted (or at least monitored) and focus on constraining what \emph{tools} can do once invoked. We add five defense-in-depth layers absent from IsolateGPT: network egress filtering, credential vaults, secret scanning, audit logging, and human-in-the-loop gates.

\textbf{Deng et al.} \citep{deng2024agents} provide a comprehensive threat taxonomy for AI agents, organizing threats along four dimensions: adversarial inputs, operational environment, untrusted external entities, and system design flaws. Our architecture directly addresses their ``operational environment'' and ``untrusted external entities'' categories through infrastructure-level enforcement, providing a concrete implementation for the defense strategies they identify as needed.

\textbf{Industry frameworks.} Several industry efforts address agent security at the framework or platform level. MAESTRO \citep{csa2025maestro} (Cloud Security Alliance) provides a multi-layer threat taxonomy for agentic AI but does not prescribe infrastructure enforcement. D{\'\i}az, Kern, and Olive \citep{google2025secureagents} (Google) outline secure agent design principles including sandboxing and least privilege but stop short of a reference implementation. SAFE-MCP \citep{safemcp2025} (Linux Foundation) defines an enterprise security assurance framework for MCP deployments. E2B \citep{e2b2025} provides Firecracker-based microVM sandboxing with sub-second cold starts, and Daytona \citep{daytona2025} offers sub-90\,ms cold start secure environments. These efforts focus on threat modeling and principles (MAESTRO, Google, SAFE-MCP) or single-layer isolation (E2B, Daytona); our work provides the multi-layer infrastructure implementation combining isolation with network filtering, credential management, audit, secret scanning, and human oversight.

\subsection{Capability-Based Security}

The capability model originates with Dennis \& Van Horn \citep{dennis1966semantics}, who proposed unforgeable tokens granting access to specific resources rather than relying on ambient authority. Saltzer \& Schroeder \citep{saltzer1975protection} formalized the principle of least privilege and complete mediation. seL4 \citep{klein2009sel4} provided the first formally verified capability-based microkernel. Capsicum \citep{watson2010capsicum} brought capability mode to FreeBSD. CHERI \citep{watson2015cheri} extends capability enforcement to hardware. Miller \citep{miller2006robust} adapted capabilities to programming language semantics, influencing WASI and Cloudflare Workers.

Our pattern adapts the capability model to the agent-tool boundary: each container receives explicit, scoped capabilities (filesystem mounts, network rules, injected credentials) rather than inheriting the host's ambient access. The MCP Gateway serves as the capability enforcement point.

\subsection{Gap in Existing Work}

\textbf{Commercial MCP Gateways.}
We surveyed 14 commercial and open-source MCP gateway products\footnote{Acuvity, Arcade AI, Azure MCP Gateway, Docker MCP Gateway, Envoy AI Gateway, Lasso Security, MCP Gateway (OSS), Prompt Security, Solo.io, Speakeasy, and four others; full methodology in the project repository.}. The dominant pattern covers authentication/authorization (OAuth 2.1, OIDC, SAML) and audit logging well. However: only Docker MCP Gateway offers container-per-tool isolation; network egress filtering per tool is absent from all but Azure (VNet-level, not per-tool); secret scanning of tool responses is nascent; and no single product combines all six defense layers. The OWASP MCP Top 10 \citep{owasp2025mcptop10} ranks ``Token Mismanagement and Secret Exposure'' as the \#1 risk.

\textbf{Prompt Injection and HITL Limitations.}
Indirect prompt injection \citep{greshake2023injection} poses a fundamental challenge: InjecAgent \citep{zhan2024injecagent} showed 24\% of agents follow injected instructions; AgentDojo \citep{debenedetti2024agentdojo} found even the best defenses fail against adaptive attacks; Lies-in-the-Loop \citep{checkmarx2025lies} demonstrated prompt injection can manipulate HITL approval dialogs; and Zhan et al.\ \citep{zhan2025adaptive} broke all 8 tested prompt-level defenses with ${>}50$\% success rate.

No existing framework, protocol, commercial product, or academic work provides a complete infrastructure-level security architecture for agent tool execution that combines all six defense layers. This is the gap we address.

% ============================================================
\section{Threat Model}
\label{sec:threat-model}
% ============================================================

\subsection{Attacker Model}

We consider three classes of adversaries:

\textbf{A1: Malicious External Input (Prompt Injection).} The attacker controls content the agent processes (e.g., web pages, emails, GitHub issues). Goal: trick the agent into executing unintended tool calls. Example: GitHub MCP breach (May 2025)---prompt injection via public issues hijacked agent to extract private repo data.

\textbf{A2: Compromised or Malicious Tool (Supply Chain).} The attacker controls an MCP server the agent connects to. Goal: exfiltrate data, execute arbitrary code on host, pivot to other tools. Example: Smithery registry vulnerability (disclosed Oct 2025)---path traversal could have exposed Docker credentials for 3{,}000+ servers; no evidence of exploitation \citep{gitguardian2025smithery,authzed2025timeline}.

\textbf{A3: Insider Threat (Over-Privileged Agent).} A legitimate agent with excessive permissions. Goal: unintended data access, credential leakage, destructive operations. Example: Supabase Cursor agent processed support tickets with service-role access, enabling SQL injection.

\subsection{Threats Addressed}

\Cref{tab:threats} maps threats to defense layers and real-world breaches.

\begin{table}[htbp]
\centering
\caption{Threats addressed by the Capability-Container Pattern.}
\label{tab:threats}
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{ID} & \textbf{Threat} & \textbf{Defense Layer} & \textbf{Breach Prevented} \\
\midrule
T1 & Malicious shell commands      & Container + HITL            & MCP Inspector (Jun '25) \\
T2 & Credential leakage in output  & Secret scanning + redaction & GitHub MCP (May '25) \\
T3 & Network data exfiltration     & Per-container egress filter & WhatsApp MCP (Apr '25) \\
T4 & Prompt injection $\to$ misuse & Risk classification + HITL  & Supabase Cursor \\
T5 & Unauthorized credential access& Vault-based injection       & mcp-remote (Jul '25) \\
T6 & Lateral movement              & Separate containers         & Smithery (Oct '25) \\
T7 & Audit trail tampering         & Append-only SQLite + WAL    & --- \\
T8 & Supply chain compromise       & Container + network policy  & Postmark (Sep '25) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Threats Not Addressed (v1)}

\begin{itemize}[nosep]
  \item \textbf{T9:} Compromised host OS---we assume a trusted host
  \item \textbf{T10:} Container image supply chain---image signing planned for v2
  \item \textbf{T11:} Side-channel attacks on shared hardware
  \item \textbf{T12:} Sophisticated ML-based evasion of content filters
  \item \textbf{T13:} Multi-agent coordination attacks---future work
\end{itemize}

\subsection{Security Properties}

\textbf{P1 (Least Privilege):} Each container receives only the capabilities required for its tool---filesystem mount points, network allowlist, and injected credentials.

\textbf{P2 (Mediated Access):} No direct agent-to-tool communication. All requests flow through the gateway, enabling policy enforcement, audit, secret scanning, and human approval.

\textbf{P3 (Defense in Depth):} Six independent layers, each providing value if others fail: container isolation, network filtering, credential vaults, audit logging, secret scanning, and HITL gates.

\textbf{P4 (Fail-Secure):} Network defaults to block; HITL auto-denies on timeout (60\,s); containers run non-root with minimal capabilities.

% ============================================================
\section{The Capability-Container Pattern}
\label{sec:pattern}
% ============================================================

\subsection{Pattern Overview}

The Capability-Container Pattern is an architectural pattern for securing autonomous AI agent tool execution. Its core invariant:

\begin{quote}
\emph{An agent NEVER directly accesses a tool. All tool invocations flow through a mediation gateway into isolated containers, each provisioned with only the capabilities required for that specific tool.}
\end{quote}

\subsection{Architecture}

\Cref{fig:architecture} illustrates the system architecture. The agent container communicates exclusively with the MCP Gateway via JSON-RPC 2.0. The gateway routes validated requests to per-tool capability containers, each configured with scoped filesystem mounts, network policies, and runtime-injected credentials. A credential vault provides dynamic secrets, and an append-only audit database records all operations with automatic secret redaction.

\begin{figure}[htbp]
\centering
\begin{verbatim}
+---------------------------------------------------------------+
|                        HOST SYSTEM                            |
|                                                               |
|  +-----------+                                                |
|  | Agent     |--JSON-RPC 2.0-->+-----------------+            |
|  | Container |                 |  MCP Gateway    |            |
|  | (isolated)|<----------------|  1. Validate    |            |
|  +-----------+                 |  2. Classify    |            |
|                                |  3. HITL gate   |            |
|                                |  4. Route       |            |
|                                |  5. Audit       |            |
|                                |  6. Scan        |            |
|                                +--+-+-+-+--------+            |
|                                   | | | |                     |
|              +--------------------+ | | +-------------+       |
|              v            v        v v                v       |
|  +----------+ +--------+ +--------+ +---------+              |
|  | Browser  | |Filesys | |CodeExec| |WebSearch|              |
|  | net:*.io | |fs:/x:ro| |fs:/tmp | |net:*.api|              |
|  | pre-auth | |net:none| |gVisor  | |no-fs    |              |
|  +----------+ +--------+ +--------+ +---------+              |
|                                                               |
|  +--[ Credential Vault (HashiCorp Vault / SOPS) ]----------+ |
|  +--[ Audit Database (SQLite + WAL, append-only) ]----------+ |
+---------------------------------------------------------------+
\end{verbatim}
\caption{Capability-Container Pattern architecture. Each tool runs in an isolated container with scoped capabilities. All communication flows through the MCP Gateway.}
\label{fig:architecture}
\end{figure}

\subsection{Components}

\textbf{Agent Container.} Runs the ReAct loop and LLM inference. Only network access: the MCP Gateway endpoint (\texttt{127.0.0.1:8100}). Cannot reach host filesystem, other containers, or external network.

\textbf{MCP Gateway.} The single enforcement point, executing the pipeline in \Cref{alg:gateway} for every tool invocation.

\textbf{Capability Containers.} Each tool runs in a Docker container configured with explicit filesystem mounts (ro/rw), domain allowlist with default-deny, environment-variable credentials cleaned on stop, CPU/memory/PID resource limits, non-root execution (UID 1000), and optional gVisor runtime (${\sim}70$ syscalls vs.\ ${\sim}300$).

\textbf{Credential Vault.} Production: HashiCorp Vault with AppRole auth, dynamic secrets, auto-renewal. Team: SOPS with age/GPG encryption. Development: environment variable fallback. Credentials are never stored in configuration files, code, or container images.

\subsection{Gateway Pipeline}

\begin{algorithm}[htbp]
\caption{MCP Gateway Request Pipeline}
\label{alg:gateway}
\begin{algorithmic}[1]
\Require $\mathit{req}$: JSON-RPC 2.0 request from agent
\Ensure $\mathit{res}$: sanitized response or denial
\State $\textsc{Validate}(\mathit{req})$ \textbf{or return} $\mathit{error}(\text{INVALID\_REQUEST})$
\State $\mathit{tool} \gets \mathit{req}.\text{params.name}$
\State $\mathit{args} \gets \mathit{req}.\text{params.arguments}$
\State $\mathit{cid} \gets \textsc{UUID}()$ \Comment{correlation ID}
\Statex
\State $\mathit{risk} \gets \textsc{ClassifyRisk}(\mathit{tool}, \mathit{args})$ \Comment{$\to \{\text{LOW, MED, HIGH, CRIT}\}$}
\Statex
\If{$\mathit{risk} \geq \text{HITL\_THRESHOLD}$}
  \State $\mathit{decision} \gets \textsc{AwaitApproval}(\mathit{tool}, \mathit{args}, \mathit{risk}, \tau{=}60\text{s})$
  \If{$\mathit{decision} \neq \text{APPROVED}$}
    \State $\textsc{LogAudit}(\mathit{cid}, \mathit{req}, \text{DENIED}, \mathit{risk})$
    \State \Return $\mathit{error}(\text{DENIED\_BY\_HITL})$
  \EndIf
\EndIf
\Statex
\State $\mathit{endpoint} \gets \text{ROUTE\_TABLE}[\mathit{tool}]$
\If{$\mathit{endpoint} = \varnothing$} \Return $\mathit{error}(\text{UNKNOWN\_TOOL})$
\EndIf
\Statex
\State $\mathit{creds} \gets \textsc{Vault.Fetch}(\mathit{tool}.\text{credential\_ref})$
\State $\textsc{InjectEnv}(\mathit{endpoint}.\text{container}, \mathit{creds})$
\Statex
\If{$\textsc{EgressFilter}(\mathit{endpoint}, \mathit{args}) = \text{BLOCKED}$}
  \State $\textsc{LogAudit}(\mathit{cid}, \mathit{req}, \text{BLOCKED\_EGRESS})$
  \State \Return $\mathit{error}(\text{EGRESS\_DENIED})$
\EndIf
\Statex
\State $\mathit{raw\_res} \gets \textsc{HttpPost}(\mathit{endpoint}, \mathit{args}, \tau{=}30\text{s})$
\Statex
\State $\mathit{secrets} \gets \textsc{ScanSecrets}(\mathit{raw\_res})$
\If{$\mathit{secrets} \neq \varnothing$}
  \State $\mathit{raw\_res} \gets \textsc{Redact}(\mathit{raw\_res}, \mathit{secrets})$
  \State $\textsc{LogAlert}(\mathit{cid}, \mathit{secrets})$
\EndIf
\Statex
\State $\textsc{LogAudit}(\mathit{cid}, \mathit{req}, \mathit{raw\_res}, \mathit{risk}, \mathit{secrets})$
\State \Return $\mathit{raw\_res}$
\end{algorithmic}
\end{algorithm}

\subsection{Security Properties (Design Goals)}

The following properties are design goals enforced through infrastructure configuration. They have not been formally verified through model checking or theorem proving; formal verification is listed as future work (\Cref{sec:conclusion}). We provide enforcement rationale for each property.

\textbf{Property 1 (Isolation):} For any two capability containers $C_i$ and $C_j$, there exists no communication channel between them except through the gateway.

\emph{Enforcement rationale:} Docker network namespaces with \texttt{-{}-network=none} for isolated tools (filesystem, code-exec) and dedicated bridge network for networked tools. Inter-container routing blocked by iptables rules; only the gateway can reach tool containers. Validated by network policy tests (97/98 correct classifications, 99.0\% accuracy).

\textbf{Property 2 (Least Privilege):} Each container $C_i$ is provisioned with capabilities $\text{Cap}(C_i) \subseteq \text{Cap}_{\text{total}}$, where $\text{Cap}(C_i)$ is the minimum set required for tool $T_i$.

\emph{Enforcement rationale:} Per-container capability manifests specifying filesystem mounts (ro/rw), network policies (domain allowlist), and credential references. Containers run as UID 1000 with \texttt{-{}-cap-drop=ALL} and only tool-specific capabilities added back. Resource limits (CPU, memory, PIDs) via cgroups.

\textbf{Property 3 (Complete Mediation):} Every tool invocation passes through the gateway. There is no bypass path from agent to tool.

\emph{Enforcement rationale:} The agent container's only network access is \texttt{127.0.0.1:8100} (the gateway). Tool containers have no ingress from agent. The gateway's ROUTE\_TABLE maps tool names to container endpoints; unregistered tools return UNKNOWN\_TOOL. \emph{Caveat:} a container escape (T9 in threat model) would bypass this property, which is why we recommend gVisor (reduced syscall surface from ${\sim}300$ to ${\sim}70$).

\textbf{Property 4 (Audit Completeness):} For every tool invocation $I$, there exists an audit record $A(I)$ containing the request, response, risk classification, and approval decision.

\emph{Enforcement rationale:} SQLite with WAL mode, append-only schema. \textsc{LogAudit}() is called on all code paths in Algorithm~1 (lines 9, 19, 26, 28---covering deny, egress block, alert, and success). Measured throughput: 2{,}036 ops/sec sustained over 1{,}000 events.

\textbf{Property 5 (Fail-Secure):} On timeout, connection failure, or ambiguous state, the system denies the operation rather than allowing it.

\emph{Enforcement rationale:} HITL auto-deny on timeout (configurable, default 60\,s). HTTP POST to tool containers uses 30\,s timeout. Network policy defaults to \texttt{block\_by\_default=True}. Unknown tools return error, not passthrough.

% ============================================================
\section{Implementation: Harombe}
\label{sec:implementation}
% ============================================================

Harombe is an open-source reference implementation of the Capability-Container Pattern, written in Python 3.11+. It implements all six defense-in-depth layers described in \Cref{sec:pattern}.

\subsection{Layer 1: Container Isolation}

Each tool runs in a Docker container with enforced resource limits:

\begin{lstlisting}[language=Python,caption={Container resource limit configuration.}]
@dataclass
class ResourceLimits:
    cpu_period: int = 100_000       # microseconds
    cpu_quota: int = 50_000         # 50% of one core
    memory_limit: str = "256m"      # hard limit
    memory_reservation: str = "128m" # soft limit
    pids_limit: int = 100           # max processes
\end{lstlisting}

Containers are created with non-root execution (UID 1000), read-only root filesystem (tmpfs for \texttt{/tmp}), explicit bind mounts only, and health monitoring via periodic \texttt{/health} endpoint checks.

\subsection{Layer 2: Network Egress Filtering}

\begin{lstlisting}[language=Python,caption={Network policy configuration.}]
@dataclass
class NetworkPolicy:
    allowed_domains: list[str]    # e.g., ["*.github.com"]
    allowed_ips: list[str]        # static IPs
    allowed_cidrs: list[str]      # e.g., ["10.0.0.0/8"]
    block_by_default: bool = True # fail-secure
    allow_dns: bool = True        # port 53
    allow_localhost: bool = False  # no loopback by default
\end{lstlisting}

Features include domain allowlisting with wildcard patterns, CIDR validation, private IP blocking (RFC\,1918 ranges) to prevent SSRF, suspicious pattern detection (port scanning, DNS tunneling), and dynamic policy updates without container restart.

\subsection{Layer 3: Credential Management}

Pluggable backend architecture: HashiCorp Vault (AppRole auth, dynamic secrets), SOPS (encrypted files with age/GPG), or environment variable fallback. The agent never sees raw credentials---the gateway handles all credential lifecycle.

\subsection{Layer 4: Audit Logging}

SQLite with WAL mode achieves ${<}1$\,ms writes (0.56\,ms measured), 2{,}036 ops/sec sustained throughput, and automatic redaction of API keys, passwords, JWT tokens, credit card numbers, and private keys.

\subsection{Layer 5: Secret Scanning}

Three detection methods: (1)~pattern matching with type-specific regexes (confidence: 0.95), (2)~prefix detection for known secret formats (confidence: 0.85), and (3)~Shannon entropy analysis (threshold: $\geq 3.5$ bits/character). Overlapping matches keep the highest-confidence detection.

\subsection{Layer 6: Human-in-the-Loop (HITL) Gates}

The HITL subsystem comprises a risk classifier (LOW/MEDIUM/HIGH/CRITICAL), an approval workflow with configurable timeout (default 60\,s auto-deny), a rules-based auto-approval engine, a historical trust manager, and a context-aware decision engine.

\subsection{Browser Security}

A novel contribution: pre-authenticated browser automation where credentials are fetched from the vault, injected into the browser context, and the agent receives a handle but never sees raw credentials. The system includes 16 risk classification rules for browser actions and accessibility-based interaction (semantic tree, not raw DOM).

\subsection{Implementation Complexity}

\Cref{tab:loc} summarizes code size by defense layer.

\begin{table}[htbp]
\centering
\caption{Implementation complexity by defense layer (lines of code, non-blank, non-comment).}
\label{tab:loc}
\small
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Layer} & \textbf{Component} & \textbf{LOC} \\
\midrule
L1: Container Isolation  & \texttt{docker\_manager.py}                  & 351   \\
L2: Network Egress       & \texttt{network.py}                          & 853   \\
L3: Credential Mgmt      & \texttt{vault.py} + \texttt{injection.py}    & 661   \\
L4: Audit Logging        & \texttt{audit\_logger.py} + \texttt{audit\_db.py} & 1{,}193 \\
L5: Secret Scanning      & \texttt{secrets.py}                          & 365   \\
L6: HITL Gates           & \texttt{hitl/} (6 modules)                   & 1{,}466 \\
Browser Security         & \texttt{browser\_*.py}                       & 569   \\
Gateway                  & \texttt{gateway.py}                          & 503   \\
\midrule
\textbf{Total}           &                                              & \textbf{5{,}961} \\
\bottomrule
\end{tabular}
\end{table}

Each concrete tool averages ${\sim}131$ LOC. The security subsystem uses 27 core Python dependencies. Adding a new tool requires a Python tool class (${\sim}50$--130 LOC), a Dockerfile, and a routing entry.

% ============================================================
\section{Evaluation \& Benchmarks}
\label{sec:evaluation}
% ============================================================

\subsection{Research Questions}

\begin{description}[nosep]
  \item[RQ1] What is the performance overhead of the Capability-Container Pattern?
  \item[RQ2] How effective is each defense layer at preventing known attack classes?
  \item[RQ3] Would the pattern have prevented real-world MCP breaches?
\end{description}

\subsection{Experimental Setup}

\textbf{Environment:} Darwin 25.2.0 arm64, Python 3.14.3 (pre-release; final release expected Oct 2026), scipy 1.17.0, scikit-learn 1.8.0, detect-secrets 1.5.0. \textbf{Statistical methods:} timing CIs via Student's $t$-distribution ($\text{df} = n{-}1$) with IQR-based outlier filtering (factor = 1.5); detection rates via Clopper--Pearson exact binomial 95\% CIs; 50 warmup iterations before each measurement loop; GC disabled during measurement. \textbf{Date:} 2026-02-11.

\subsubsection{Evaluation Methodology and Scope}

All benchmarks use \textbf{synthetic test data created alongside the implementation}. The 1{,}534-sample corpus was constructed to exercise specific regex patterns and detection heuristics, not sampled from production environments. Breach scenarios are \textbf{reconstructed from public incident reports} (Invariant Labs, Checkmarx, Orca Security, Trail of Bits), not replays of actual attack traffic. No external red team or third-party adversarial testing was conducted. Results represent \textbf{defense coverage mapping}---demonstrating which attack classes each security layer addresses---not adversarial robustness guarantees.

\subsubsection{Benchmark Infrastructure}

All benchmarks exercise the real security modules directly (no Docker, no mocking). Performance benchmarks measure \textbf{gateway logic overhead only}---the time spent in security module Python code paths. They do not include container-level overhead (Docker image pull, container creation, health check, HTTP round-trip). For cross-run reproducibility, \texttt{benchmarks/run\_multi.py} invokes the suite $N$ times independently and computes cross-run 95\% CIs using the $t$-distribution over per-run means.

\subsubsection{Statistical Methodology Notes}

Large sample sizes ($n \geq 200$) in performance benchmarks justify the Central Limit Theorem for $t$-based confidence intervals without formal normality testing. IQR-based outlier filtering (factor = 1.5) removes measurement artifacts (GC pauses, OS scheduling jitter); we acknowledge this may underestimate tail latency for workloads sensitive to worst-case performance. No family-wise error correction (e.g., Bonferroni) is applied across multiple metrics; readers should interpret individual CIs accordingly. Independence is justified by separate function calls with fresh inputs and separate process invocations for cross-run measurements.

\subsection{Performance Benchmarks (RQ1)}

\subsubsection{Secret Scanning Latency by Message Size}

\begin{table}[htbp]
\centering
\caption{Secret scanning latency by message size ($n=200$ per size, IQR-filtered).}
\label{tab:scan-latency}
\small
\begin{tabular}{@{}rrrrrr@{}}
\toprule
\textbf{Size (chars)} & \textbf{Mean (ms)} & \textbf{Filtered (ms)} & \textbf{95\% CI} & \textbf{p95 (ms)} & \textbf{Outliers} \\
\midrule
100    & 0.008 & 0.008 & [0.008, 0.008] & 0.008 & 26 \\
500    & 0.030 & 0.029 & [0.029, 0.029] & 0.030 & 44 \\
1{,}000 & 0.057 & 0.056 & [0.056, 0.056] & 0.061 & 11 \\
5{,}000 & 0.317 & 0.317 & [0.312, 0.321] & 0.371 & 1  \\
10{,}000& 0.553 & 0.553 & [0.552, 0.555] & 0.573 & 14 \\
\bottomrule
\end{tabular}
\end{table}

Sub-millisecond scanning up to 10K characters. Linear scaling (${\sim}0.055$\,ms per 1K chars) confirms $O(n)$ regex-based approach.

\subsubsection{Full Pipeline Overhead}

\begin{table}[htbp]
\centering
\caption{Gateway logic overhead---all security layers combined ($n=200$, 25 outliers removed). Does not include container-level latency.}
\label{tab:pipeline}
\small
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Mean           & 0.024\,ms \\
Filtered mean  & 0.023\,ms [0.023, 0.023] \\
p50            & 0.023\,ms \\
p95            & 0.024\,ms \\
p99            & 0.024\,ms \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Baseline comparison:} bare operations (no security) take 0.000384\,ms; the full gateway logic pipeline takes 0.023\,ms---an absolute overhead of ${\sim}0.024$\,ms, or \textbf{0.005\% of a typical 500\,ms LLM inference call}. Note: this measures Python security module code paths only; production deployments add container-level latency (creation, HTTP round-trip, health check).

\subsection{Detection Effectiveness (RQ2)}

\subsubsection{Secret Scanner True Positive Rate}

Corpus: 900 synthetic secrets (100 per type, 9 types), conforming to production regex patterns. 20 hand-crafted samples per type serve as baseline; 80 additional per type generated programmatically via deterministic SHA-256 seeded hashing (\Cref{tab:tp-rate}).

\begin{table}[htbp]
\centering
\caption{Secret scanner true positive rate by type ($n=100$ per type, Clopper--Pearson CIs).}
\label{tab:tp-rate}
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Secret Type} & \textbf{Samples} & \textbf{Detected} & \textbf{TP Rate} & \textbf{95\% CI} \\
\midrule
AWS keys       & 100 & 100 & 100\% & [0.964, 1.000] \\
GitHub tokens  & 100 & 100 & 100\% & [0.964, 1.000] \\
Slack tokens   & 100 & 100 & 100\% & [0.964, 1.000] \\
Stripe keys    & 100 & 100 & 100\% & [0.964, 1.000] \\
JWT tokens     & 100 & 100 & 100\% & [0.964, 1.000] \\
Private keys   & 100 & 100 & 100\% & [0.964, 1.000] \\
Database URLs  & 100 & 100 & 100\% & [0.964, 1.000] \\
Passwords      & 100 & 100 & 100\% & [0.964, 1.000] \\
API keys       & 100 & 100 & 100\% & [0.964, 1.000] \\
\midrule
\textbf{Overall} & \textbf{900} & \textbf{900} & \textbf{100\%} & \textbf{[0.996, 1.000]} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Secret Scanner False Positive Rate}

Corpus: 514 benign samples across 7 categories (\Cref{tab:fp-rate}). The single false positive was a placeholder token (\texttt{ghp\_xxxx\ldots}) matching the GitHub token pattern---arguably a true positive since the pattern is structurally valid.

\begin{table}[htbp]
\centering
\caption{Secret scanner false positive rate by category (Clopper--Pearson CIs).}
\label{tab:fp-rate}
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Category} & \textbf{Samples} & \textbf{FPs} & \textbf{FP Rate} & \textbf{95\% CI} \\
\midrule
Prose       & 80  & 0 & 0.0\% & [0.000, 0.045] \\
Code        & 70  & 0 & 0.0\% & [0.000, 0.051] \\
Logs        & 70  & 0 & 0.0\% & [0.000, 0.051] \\
Config      & 71  & 0 & 0.0\% & [0.000, 0.051] \\
URLs        & 70  & 0 & 0.0\% & [0.000, 0.051] \\
Hashes      & 71  & 0 & 0.0\% & [0.000, 0.051] \\
Edge cases  & 82  & 1 & 1.2\% & [0.000, 0.066] \\
\midrule
\textbf{Overall} & \textbf{514} & \textbf{1} & \textbf{0.19\%} & \textbf{[0.000, 0.011]} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Comparative Detection: Harombe vs.\ detect-secrets}

Both tools evaluated on the same 1{,}414-sample corpus (\Cref{tab:comparison}).

\begin{table}[htbp]
\centering
\caption{Comparative detection: Harombe vs.\ detect-secrets 1.5.0 (1{,}414 samples).}
\label{tab:comparison}
\small
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Metric} & \textbf{Harombe} & \textbf{detect-secrets} \\
\midrule
True Positives   & 900 & 900 \\
False Positives  & 17  & 512 \\
True Negatives   & 497 & 2   \\
False Negatives  & 0   & 0   \\
Precision        & \textbf{0.981} & 0.637 \\
Recall           & 1.000 & 1.000 \\
$F_1$ Score      & \textbf{0.991} & 0.779 \\
\bottomrule
\end{tabular}
\end{table}

Harombe achieves a 30$\times$ reduction in false positives while maintaining perfect recall.

\subsubsection{Adversarial Secret Detection}

\begin{table}[htbp]
\centering
\caption{Adversarial secret detection robustness (120 samples, 30 per technique, Clopper--Pearson CIs).}
\label{tab:adversarial}
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Evasion Technique} & \textbf{n} & \textbf{Detected} & \textbf{Rate} & \textbf{95\% CI} \\
\midrule
Multiline context (YAML/env/Docker/CI) & 30 & 30 & 100\%  & [0.884, 1.000] \\
Split secrets (multi-line/variable)     & 30 & 28 & 93.3\% & [0.779, 0.992] \\
Unicode homoglyphs (Cyrillic/Greek/ZWSP)& 30 & 10 & 33.3\% & [0.173, 0.528] \\
Base64-encoded (single/double/nested)   & 30 & 1  & 3.3\%  & [0.001, 0.172] \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Risk Classification}

50 test cases across 4 risk levels yield a perfect confusion matrix (50/50 accuracy). Unknown tools default to MEDIUM (conservative).

\subsection{Breach Prevention Analysis (RQ3)}

Each breach scenario was reconstructed from public incident reports and tested against the defense layers (\Cref{tab:breaches}). Attack payloads are synthetic reconstructions, not replays of actual attack traffic.

\begin{table}[htbp]
\centering
\caption{Breach scenario coverage: 2025 MCP attack patterns reconstructed against Harombe.}
\label{tab:breaches}
\small
\begin{tabular}{@{}clcl@{}}
\toprule
\textbf{ID} & \textbf{Attack Scenario} & \textbf{Blocked?} & \textbf{Layers Activated} \\
\midrule
T1 & WhatsApp exfiltration       & \textbf{Yes} & scanner, egress, HITL \\
T2 & GitHub credential leak      & \textbf{Yes} & scanner \\
T3 & mcp-remote cmd injection    & \textbf{Yes} & classifier, HITL \\
T4 & Filesystem path traversal   & No$^*$       & classifier \\
T5 & Smithery Docker cred theft  & \textbf{Yes} & scanner, redactor \\
T6 & Postmark BCC injection      & \textbf{Yes} & HITL, redactor \\
T7 & Multi-layer compound        & \textbf{Yes} & all 5 layers \\
\bottomrule
\multicolumn{4}{@{}l}{\footnotesize $^*$Path traversal prevention relies on L1 container bind mounts, not risk classification.}
\end{tabular}
\end{table}

6/7 attack scenarios addressed at the security module level. T7 demonstrates defense-in-depth: all 5 independent layers activated simultaneously. These results reflect defense \emph{coverage}---which layers activate for which attack classes---not adversarial resilience against adaptive attackers.

\subsection{Layer-by-Layer Coverage}

\begin{table}[htbp]
\centering
\caption{Defense layer coverage by attack vector. $\bullet\bullet$ = primary defense, $\bullet$ = detection/forensics.}
\label{tab:coverage}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Attack Vector} & \textbf{L1} & \textbf{L2} & \textbf{L3} & \textbf{L4} & \textbf{L5} & \textbf{L6} \\
\midrule
Malicious shell commands      & $\bullet\bullet$ &                   &                   & $\bullet$ &                   & $\bullet\bullet$ \\
Credential leakage            &                   &                   & $\bullet\bullet$ & $\bullet$ & $\bullet\bullet$ &                   \\
Network exfiltration          & $\bullet$         & $\bullet\bullet$ &                   & $\bullet$ &                   &                   \\
Prompt injection $\to$ misuse &                   &                   &                   & $\bullet$ &                   & $\bullet\bullet$ \\
Unauthorized cred access      & $\bullet\bullet$ &                   & $\bullet\bullet$ & $\bullet$ &                   &                   \\
Lateral movement              & $\bullet\bullet$ & $\bullet\bullet$ &                   & $\bullet$ &                   &                   \\
Supply chain compromise       & $\bullet\bullet$ & $\bullet\bullet$ &                   & $\bullet$ & $\bullet\bullet$ &                   \\
\bottomrule
\end{tabular}
\end{table}

Every attack vector is covered by at least 2 layers. No single layer failure results in undetected compromise.

\subsection{Cross-Run Reproducibility}

We ran the full benchmark suite 5 independent times. Key cross-run results in \Cref{tab:crossrun}.

\begin{table}[htbp]
\centering
\caption{Cross-run reproducibility (5 runs, 31 metrics, $t$-distribution CIs).}
\label{tab:crossrun}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Metric} & \textbf{Mean (ms)} & \textbf{StdDev} & \textbf{95\% CI} \\
\midrule
Full pipeline              & 0.025 & 0.002 & [0.022, 0.028] \\
Audit write                & 0.523 & 0.034 & [0.481, 0.565] \\
Secret scan (100 chars)    & 0.008 & 0.001 & [0.007, 0.009] \\
Secret scan (1K chars)     & 0.057 & 0.001 & [0.055, 0.058] \\
Secret scan (10K chars)    & 0.556 & 0.023 & [0.528, 0.584] \\
Secret redaction (e2e)     & 0.094 & 0.005 & [0.088, 0.099] \\
Egress (blocked domain)    & 13.918 & 0.843 & [12.871, 14.964] \\
\bottomrule
\end{tabular}
\end{table}

All security-critical paths show tight cross-run CIs. The blocked domain egress check shows the highest variance (13.9\,ms $\pm$ 0.8\,ms) due to DNS resolution timing.

\subsection{Limitations of Evaluation}
\label{sec:eval-limitations}

\begin{itemize}[nosep]
  \item \textbf{Self-benchmarked evaluation:} all benchmarks use synthetic test data created alongside the implementation. No external red team or third-party adversarial testing was conducted. Results represent defense coverage, not adversarial robustness guarantees.
  \item Benchmarks run on a single machine (Apple Silicon), not production deployment.
  \item Secret corpus (900 TP + 514 benign = 1{,}414 samples) uses synthetic secrets; real-world FP rates may differ with diverse codebases.
  \item \textbf{Gateway logic overhead} (0.025\,ms) measures Python security module code paths only. Production overhead includes container creation, HTTP round-trip, and network latency.
  \item Base64-encoded secrets (3.3\% detection) and Unicode homoglyphs (33.3\%) evade regex-based scanning---consistent with industry limitations \citep{basak2023secrets,orca2024base64}.
  \item Performance under high concurrency (100+ agents) untested.
  \item T4 (path traversal) cannot be demonstrated without Docker in the benchmark suite.
  \item No user study or developer experience evaluation conducted.
\end{itemize}

% ============================================================
\section{Discussion \& Limitations}
\label{sec:discussion}
% ============================================================

\subsection{Why Infrastructure Over Protocol}

The recurring pattern in MCP breaches is clear: protocol-level trust is insufficient. This parallels the evolution of web security---HTTP alone cannot prevent XSS, CSRF, or injection attacks; infrastructure controls are required. Similarly, MCP alone cannot prevent tool poisoning, credential leakage, or data exfiltration.

\emph{The protocol tells you \textbf{how} to talk to tools. The infrastructure determines \textbf{what tools can do}.}

\subsection{Trade-offs}

\textbf{Performance vs.\ Security.} Container isolation adds latency. Our benchmarks show ${<}5$\,ms for warm containers, but cold starts remain a concern. Mitigation: pre-warming, connection pooling, health check-based readiness probes.

\textbf{Operational Complexity.} Docker is a hard production dependency. Mitigation: development mode runs without containers; production automated via Docker Compose or Kubernetes.

\textbf{Flexibility vs.\ Control.} Strict egress filtering may break tools with dynamic network needs. Mitigation: wildcard support, dynamic policy updates, HITL approval for non-allowlisted domains.

\subsection{Limitations}

\textbf{Host OS Trust Assumption.} If an attacker has root on the host, container isolation provides no protection. Future work: hardware-enforced isolation (Intel SGX, AMD SEV-SNP).

\textbf{Container Escape.} Recent CVEs underscore this risk: CVE-2024-1086 (99.4\% success on KernelCTF images \citep{cve2024_1086poc}), CVE-2025-23266 (CVSS 9.0), and three runc escapes in November 2025. Lin et al.\ \citep{lin2018container} found 56.82\% of exploits succeed against default Docker. gVisor mitigates this \citep{jang2022gvisor} but introduces compatibility issues. This reinforces defense-in-depth.

\textbf{Adversarial Evasion.} Regex-based scanning is vulnerable to encoding-based evasion: base64-encoded secrets detected at only 3.3\% (CI: [0.001, 0.172]) and Unicode homoglyphs at 33.3\% (CI: [0.173, 0.528]). This is consistent with industry limitations \citep{basak2023secrets,orca2024base64,boucher2023trojan}. Defense paths: Unicode normalization (NFKC) and ML-based scanning \citep{basak2025llmsecrets,wiz2025scanning}.

\textbf{Prompt Injection and HITL Limitations.} Lies-in-the-Loop \citep{checkmarx2025lies} shows prompt injection can manipulate HITL dialogs. Zhan et al.\ \citep{zhan2025adaptive} broke all 8 tested prompt-level defenses. Our response: HITL is one of six layers; infrastructure layers operate independently of prompt-level trust.

\textbf{Supply Chain Risk.} DockerDash \citep{dockerdash2025} demonstrated AI-specific container image compromise. Image signing (Sigstore/cosign) is planned.

\textbf{Single Point of Failure and Compromise.} The gateway is both a single point of failure and a single point of compromise: if subverted, all five security properties collapse simultaneously. This is the inverse of the defense-in-depth principle applied to tools. Gateway hardening: minimal API surface (single JSON-RPC endpoint), no tool-specific logic in the gateway code path, non-root execution, single-purpose process. The audit database should ideally write to a separate trust domain (remote syslog, append-only S3) so that audit integrity survives gateway compromise. Future work: replication and hardware-backed attestation.

\textbf{Scalability.} The current implementation targets single-host deployment. Multi-host distributed gateways are future work.

\textbf{Forward Compatibility with MCP Spec Evolution.}
The MCP specification is actively evolving. If future versions add native authentication or isolation directives, our pattern adapts naturally: the gateway can consume MCP-level security metadata as additional policy inputs while continuing to enforce infrastructure-level boundaries.

\subsection{Ethical Considerations}

The Capability-Container Pattern is defensive technology. The same techniques could theoretically sandbox agents for malicious purposes, but we believe the defensive value significantly outweighs this risk given demonstrated real-world harm from unsecured deployments.

\subsection{Comparison with MicroVM Approaches}

MicroVMs (Firecracker) provide stronger isolation (separate kernel) at the cost of higher resources \citep{agache2020firecracker}. However, Weissman et al.\ \citep{weissman2023firecracker} showed limited Spectre/MDS protection, and Xiao et al.\ \citep{xiao2023microvm} demonstrated exploitable operation forwarding. Our architecture is runtime-agnostic: the same pattern works with Docker, gVisor, or microVMs.

\subsection{Comparison with Commercial MCP Gateways}

Our survey of 14 commercial and open-source MCP gateway products reveals the market is crowded but shallow on defense-in-depth. Container-per-tool isolation: only Docker MCP Gateway. HITL gates: only 3 products (none combined with containers). Secret scanning: nascent (PII only). Network egress filtering per tool: effectively absent. This gap is structural: existing products evolved from API gateway heritage and lack infrastructure-level security primitives.

% ============================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================

We presented the Capability-Container Pattern, an infrastructure-level security architecture for autonomous AI agent tool execution. The pattern enforces six defense-in-depth layers ensuring that even compromised tools cannot access the host, leak credentials, exfiltrate data, or interfere with other tools.

\subsection{Key Results}

\begin{itemize}[nosep]
  \item \textbf{Defense coverage for 6/7 reconstructed 2025 MCP breach scenarios} (synthetic reconstructions from public incident reports; no external red team)
  \item \textbf{${\sim}0.025$\,ms gateway logic overhead} (cross-run 95\% CI: [0.022, 0.028])---0.005\% of a 500\,ms LLM inference call
  \item \textbf{100\% secret detection rate} (900/900 across 9 types, Clopper--Pearson CI: [0.996, 1.000]) with 0.19\% FP rate (1/514, CI: [0.000, 0.011])
  \item \textbf{$F_1 = 0.991$} vs.\ 0.779 for detect-secrets (30$\times$ fewer false positives on 1{,}414-sample corpus)
  \item \textbf{Sub-microsecond risk classification}
  \item \textbf{2{,}036 ops/sec audit throughput}
  \item Transparent adversarial boundaries (120 samples, 30 per technique): 100\% for multiline context, 93.3\% for split secrets, 3.3\% for base64, 33.3\% for homoglyphs
  \item \textbf{5{,}961 LOC} across 8 security layers
\end{itemize}

\subsection{Core Insight}

MCP defines \emph{how} agents communicate with tools. It does not and cannot enforce \emph{what tools are allowed to do}. This enforcement must happen at the infrastructure layer---just as web security requires infrastructure controls beyond HTTP.

\subsection{Future Work}

\begin{enumerate}[nosep]
  \item Hardware-enforced isolation (Intel SGX, AMD SEV-SNP)
  \item Container image verification (Sigstore/cosign)
  \item Multi-host distributed gateway architecture
  \item Formal verification of isolation and mediation properties
  \item Red team evaluation
  \item Multi-agent coordination security
  \item Encoding-aware scanning (base64 decoding, Unicode normalization) to address the 3.3\%/33.3\% adversarial detection gaps
  \item Concurrency benchmarks (100+ simultaneous agents)
  \item User study and developer experience evaluation
\end{enumerate}

% ============================================================
% References
% ============================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
